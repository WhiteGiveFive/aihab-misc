#!/usr/bin/env python3
"""
Generated by ChatGPT
compute_human_accuracy.py

Compute Top-1 and Top-3 accuracy of a human expert's habitat classifications
against ground-truth labels.
"""

import argparse
import pandas as pd
import warnings
import sys

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import numpy as np

def load_table(path: str) -> pd.DataFrame:
    """
    Load a table from CSV or TSV. Try CSV first, then TSV.
    """
    try:
        return pd.read_csv(path)
    except Exception:
        try:
            return pd.read_csv(path, sep='\t')
        except Exception as e:
            print(f"Failed to read {path} as CSV or TSV: {e}", file=sys.stderr)
            sys.exit(1)

def draw_cm(matched_table: pd.DataFrame) -> plt.Figure:
    # -- compute confusion matrix for Top-1 --
    y_true = matched_table["true_label"]
    y_pred = matched_table["top1"]
    labels = sorted(set(y_true) | set(y_pred))
    cm = confusion_matrix(y_true, y_pred, labels=labels)

    # 2) Normalize each row (true-label) so it sums to 1
    #    Add a small epsilon to avoid division by zero if a class has zero samples
    row_sums = cm.sum(axis=1, keepdims=True) + 1e-12
    cm_normalized = cm.astype(float) / row_sums

    # 3) Plot the normalized matrix with a larger figure
    fig, ax = plt.subplots(figsize=(12, 10))  # <-- make the figure bigger
    im = ax.imshow(cm_normalized, vmin=0, vmax=1, interpolation='nearest')
    ax.set_title("Row-Normalized Confusion Matrix")
    ax.set_xlabel("Predicted label")
    ax.set_ylabel("True label")

    # Tick labels
    ax.set_xticks(np.arange(len(labels)))
    ax.set_yticks(np.arange(len(labels)))
    ax.set_xticklabels(labels, rotation=45, ha='right')
    ax.set_yticklabels(labels)

    # Annotate each cell with a decimal (not percentage)
    for i in range(len(labels)):
        for j in range(len(labels)):
            val = cm_normalized[i, j]
            ax.text(j, i, f"{val:.2f}", ha='center', va='center')

    # Add colorbar
    cbar = fig.colorbar(im, ax=ax)
    cbar.set_label("Proportion of true-class samples")

    plt.tight_layout()
    plt.show()

def main():
    p = argparse.ArgumentParser(
        description="Compute Top-1 and Top-3 accuracy for human annotations."
    )
    p.add_argument(
        "--human", "-H", required=True,
        help="Path to human expert table (CSV/TSV)."
    )
    p.add_argument(
        "--truth", "-T", required=True,
        help="Path to ground-truth table (CSV/TSV)."
    )
    args = p.parse_args()

    # Load data
    human_df = load_table(args.human)
    truth_df = load_table(args.truth)

    # We assume these column names; if yours differ, adjust here:
    #   human_df: file_name, Top 1 Label, Top 2 Label, Top 3 Label
    #   truth_df: file_name, plot_labels (code), label (text)
    #
    # Normalize column names
    human_df = human_df.rename(columns={
        "Source Name": "file_name",
        "Top 1 Label": "top1",
        "Top 2 Label": "top2",
        "Top 3 Label": "top3",
    })
    truth_df = truth_df.rename(columns={
        "label": "true_label",
    })

    # Merge
    merged = human_df.merge(
        truth_df[["file_name", "true_label"]],
        on="file_name", how="left", indicator=True
    )

    # Warn about any human samples missing in ground truth
    missing = merged[merged["_merge"] == "left_only"]
    if not missing.empty:
        warnings.warn(
            f"{len(missing)} samples in the human table were not found "
            f"in the ground-truth table and will be skipped:\n"
            f"{missing['file_name'].tolist()}"
        )
    # Keep only matched rows
    matched = merged[merged["_merge"] == "both"]

    if matched.empty:
        print("No overlapping samples to evaluate.", file=sys.stderr)
        sys.exit(1)

    # Compute Top-1 correctness
    matched["correct_top1"] = matched["top1"] == matched["true_label"]

    # Compute Top-3 correctness
    def in_top3(row):
        return row["true_label"] in (row["top1"], row["top2"], row["top3"])
    matched["correct_top3"] = matched.apply(in_top3, axis=1)

    # Aggregate
    total = len(matched)
    top1_acc = matched["correct_top1"].sum() / total
    top3_acc = matched["correct_top3"].sum() / total

    # Output
    print(f"Evaluated {total} human‐annotated samples.")
    print(f"Top-1 accuracy: {matched['correct_top1'].sum()} / {total} = {top1_acc:.2%}")
    print(f"Top-3 accuracy: {matched['correct_top3'].sum()} / {total} = {top3_acc:.2%}")

    # Draw CM
    draw_cm(matched)

if __name__ == "__main__":
    main()